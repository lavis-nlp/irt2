{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82c2827-6a48-4591-a130-238ef9f48392",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# KGC Control Experiments\n",
    "\n",
    "We run control experiments to check correctness of metric calculation,\n",
    "and get a approximate performance boundary for chat based llms which propose mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2365c8cb-1c9b-4df9-a990-fcd7678c900b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import irt2\n",
    "\n",
    "p_data = irt2.ENV.DIR.DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efddd0f9-fc61-42ec-a520-815762bb79fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from irt2.types import Split, Task, Sample, MID, RID, VID\n",
    "from irt2.dataset import IRT2\n",
    "from irt2.evaluation import Predictions\n",
    "\n",
    "import random\n",
    "from typing import Iterable, Literal\n",
    "\n",
    "\n",
    "Tasks = dict[tuple[MID, RID], set[VID]]\n",
    "\n",
    "\n",
    "def true_vids(tasks: Tasks, ds: IRT2, **_) -> Predictions:\n",
    "    \"\"\"This model cheats and always answers always correctly.\"\"\"\n",
    "    for (mid, rid), vids in tasks.items():\n",
    "        yield (mid, rid), ((vid, 1) for vid in vids)\n",
    "\n",
    "def true_mentions(\n",
    "    tasks: Tasks,\n",
    "    ds: IRT2,\n",
    "    split: Literal['validation', 'test'],\n",
    "    **_,\n",
    ") -> Predictions:\n",
    "    \"\"\"This model cheats and knows the correct mentions.\"\"\"\n",
    "    splits = (Split.train, Split.valid)\n",
    "    if split == 'test':\n",
    "        splits += (Split.test, )\n",
    "\n",
    "    ids = ds.idmap\n",
    "    for (mid, rid), gt_vids in tasks.items():\n",
    "        mentions = {\n",
    "            ids.mid2str[mid]\n",
    "            for mids in map(ids.vid2mids.get, gt_vids)\n",
    "            for mid in mids\n",
    "        }\n",
    "\n",
    "        pr_vids = ds.find_by_mention(\n",
    "            *mentions,\n",
    "            splits=splits,\n",
    "        )\n",
    "\n",
    "        yield (mid, rid), ((vid, 1) for vid in pr_vids)\n",
    "\n",
    "\n",
    "def random_guessing(\n",
    "    tasks: Tasks,\n",
    "    ds: IRT2,\n",
    "    split: Literal['validation', 'test'],\n",
    "    seed: int,\n",
    "    **_,\n",
    ") -> Predictions:\n",
    "    \"\"\"This model is just guessing randomly.\"\"\"\n",
    "    rng = random.Random()\n",
    "    rng.seed(seed)\n",
    "\n",
    "    ids = ds.idmap\n",
    "    candidates = ids.split2vids[Split.train] | ids.split2vids[Split.valid]\n",
    "    if split == 'test':\n",
    "        candidates |= ids.split2vids[Split.test]\n",
    "\n",
    "    perm = list(candidates)\n",
    "    for (mid, rid), vids in tasks.items():\n",
    "        yield (mid, rid), ((vid, rng.random()) for vid in rng.sample(perm, k=100))\n",
    "\n",
    "\n",
    "\n",
    "MODELS = {\n",
    "    'true-vertices': true_vids,\n",
    "    'true-mentions': true_mentions,\n",
    "    'random-guessing': random_guessing,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92ebf34-04b8-44b4-bd0b-ef219153fec5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from irt2 import evaluation\n",
    "from ktz.collections import dflat\n",
    "\n",
    "import yaml\n",
    "from functools import partial\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def flatten(report: dict):\n",
    "    before = dict(\n",
    "        dataset=report['dataset'],\n",
    "        model=report['model'],\n",
    "        date=report['date'],\n",
    "        split=report['split'],\n",
    "    )\n",
    "\n",
    "    metrics = dflat(report['metrics'], sep=' ')\n",
    "    metrics = dict(sorted(metrics.items()))\n",
    "\n",
    "    return before | metrics\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    ds: IRT2,\n",
    "    name: str,\n",
    "    split: str,\n",
    "    head_predictions: Predictions,\n",
    "    tail_predictions: Predictions,\n",
    "):\n",
    "    metrics = evaluation.evaluate(\n",
    "        ds=ds,\n",
    "        task='kgc',\n",
    "        split=split,\n",
    "        head_predictions=head_predictions,\n",
    "        tail_predictions=tail_predictions,\n",
    "    )\n",
    "\n",
    "    return evaluation.create_report(\n",
    "        metrics,\n",
    "        ds,\n",
    "        task='kgc',\n",
    "        split=split,\n",
    "        model=name,\n",
    "        filenames=dict(notebook='ipynb/control-experiments.ipynb'),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def run(\n",
    "    ds: IRT2,\n",
    "    name: str,\n",
    "    model: Callable,\n",
    "    split: str,\n",
    "    seed: int,\n",
    "):\n",
    "    predictor = partial(\n",
    "        model,\n",
    "        ds=ds,\n",
    "        split=split,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    assert split == 'validation' or split == 'test'\n",
    "\n",
    "    if split == 'validation':\n",
    "        head_predictions = predictor(ds.open_kgc_val_heads)\n",
    "        tail_predictions = predictor(ds.open_kgc_val_tails)\n",
    "\n",
    "    if split == 'test':\n",
    "        head_predictions = predictor(ds.open_kgc_test_heads)\n",
    "        tail_predictions = predictor(ds.open_kgc_test_tails)\n",
    "\n",
    "\n",
    "    report = evaluate(\n",
    "        ds=ds,\n",
    "        name=name,\n",
    "        split=split,\n",
    "        head_predictions=head_predictions,\n",
    "        tail_predictions=tail_predictions,\n",
    "    )\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a558169f-db1d-45a2-a01e-4bdda41bf0dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write results to /home/felix/Complex/dkg/irt2/data/evaluation/control-experiments-original.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IRT2/CDE-L: 15020 vertices | 45 relations | 32666 mentions\n",
      "  validation\n",
      "    - percentage=None\n",
      "    - 1184 head and 19654 tail tasks = 20838\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.625\n",
      "  test\n",
      "    - percentage=None\n",
      "    - 2697 head and 46104 tail tasks = 48801\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IRT2/CDE-M: 15020 vertices | 45 relations | 32666 mentions\n",
      "  validation\n",
      "    - percentage=None\n",
      "    - 782 head and 23656 tail tasks = 24438\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.636\n",
      "  test\n",
      "    - percentage=None\n",
      "    - 3174 head and 94747 tail tasks = 97921\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IRT2/CDE-S: 14207 vertices | 12 relations | 28582 mentions\n",
      "  validation\n",
      "    - percentage=None\n",
      "    - 131 head and 13084 tail tasks = 13215\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.628\n",
      "  test\n",
      "    - percentage=None\n",
      "    - 513 head and 52141 tail tasks = 52654\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IRT2/CDE-T: 12389 vertices | 5 relations | 23894 mentions\n",
      "  validation\n",
      "    - percentage=None\n",
      "    - 68 head and 5299 tail tasks = 5367\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.827\n",
      "  test\n",
      "    - percentage=None\n",
      "    - 423 head and 47434 tail tasks = 47857\n",
      "    - model:  true-mentions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - result: 0.824\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from ktz.collections import dconv, dflat\n",
    "from irt2.loader import from_config_file\n",
    "\n",
    "\n",
    "def _run_all(datasets_config, models, splits, seed: int):\n",
    "    datasets = from_config_file(\n",
    "        root_path=irt2.ENV.DIR.ROOT,\n",
    "        **datasets_config,\n",
    "    )\n",
    "\n",
    "    for _, dataset in datasets:\n",
    "        print('\\n', str(dataset))\n",
    "\n",
    "        for split in splits:\n",
    "            if split == 'validation':\n",
    "                n_heads = len(dataset.open_kgc_val_heads)\n",
    "                n_tails = len(dataset.open_kgc_val_tails)\n",
    "\n",
    "            if split == 'test':\n",
    "                n_heads = len(dataset.open_kgc_test_heads)\n",
    "                n_tails = len(dataset.open_kgc_test_tails)\n",
    "\n",
    "            options = dataset.meta['loader']\n",
    "            percentage = None\n",
    "            if \"subsample\" in options:\n",
    "                percentage = options[\"subsample\"].get(split, None)\n",
    "\n",
    "            print(\n",
    "                '  ' + split,\n",
    "                f'percentage={percentage}',\n",
    "                f'{n_heads} head and {n_tails} tail tasks'\n",
    "                f' = {n_heads + n_tails}',\n",
    "                sep='\\n    - ',\n",
    "            )\n",
    "\n",
    "            meta = {\n",
    "                'percentage': percentage,\n",
    "                'total tasks': n_heads + n_tails,\n",
    "                'head tasks': n_heads,\n",
    "                'tail tasks': n_tails,\n",
    "            }\n",
    "\n",
    "            # print(', '.join(map(str, dataset.table_row)))\n",
    "            for model in models:\n",
    "                print('    - model: ', model)\n",
    "                report = run(dataset, model, MODELS[model], split, seed)\n",
    "\n",
    "                h10 = report['metrics']['all']['micro']['hits_at_10']\n",
    "                print(f'    - result: {h10:2.3f}')\n",
    "\n",
    "                yield meta | flatten(report)\n",
    "\n",
    "\n",
    "def run_all(out, datasets_config, models, splits, seed: int):\n",
    "    out.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    print(f'write results to {out}')\n",
    "    with out.open(mode='w') as fd:\n",
    "        writer = None\n",
    "\n",
    "        for flat in _run_all(datasets_config, models, splits, seed):\n",
    "            if writer is None:\n",
    "                header = ['seed'] + list(flat.keys())\n",
    "\n",
    "                writer = csv.DictWriter(fd, fieldnames=header)\n",
    "                writer.writeheader()\n",
    "\n",
    "            writer.writerow(flat | {'seed': seed})\n",
    "\n",
    "\n",
    "\n",
    "all_config = {\n",
    "    'datasets_config': {\n",
    "        'config_file': irt2.ENV.DIR.CONF / 'datasets' / 'original.yaml',\n",
    "        'without': ['blp/*'],\n",
    "        # 'config_file': irt2.ENV.DIR.CONF / 'datasets' / 'original-subsampled.yaml',\n",
    "        # 'config_file': irt2.ENV.DIR.CONF / 'datasets' / 'full.yaml',\n",
    "        # 'config_file': irt2.ENV.DIR.CONF / 'datasets' / 'full-subsampled.yaml',\n",
    "        # 'without': ('blp/wikidata5m', )\n",
    "    },\n",
    "    'models': ['true-mentions'],\n",
    "    'splits': [\n",
    "        'validation',\n",
    "        'test',\n",
    "    ],\n",
    "    'seed': 31189,\n",
    "}\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    name = config['datasets_config']['config_file'].stem\n",
    "    fcsv = f\"control-experiments-{name}.csv\"\n",
    "    run_all(out=p_data / \"evaluation\" / fcsv, **config)\n",
    "\n",
    "\n",
    "main(all_config)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1fd324-5b53-4363-ac4e-541943d6bbf1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write results to /home/felix/Complex/dkg/irt2/data/evaluation/subsample-experiments-original.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from irt2.loader import from_config_file\n",
    "\n",
    "def run_subsampling(out, datasets_config, percentages: Iterable[float], seed: int):\n",
    "    out.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    print(f'write results to {out}')\n",
    "    with out.open(mode='w') as fd:\n",
    "        writer = None\n",
    "\n",
    "        datasets = from_config_file(\n",
    "            root_path=irt2.ENV.DIR.ROOT,\n",
    "            **datasets_config,\n",
    "        )\n",
    "\n",
    "        for _, dataset in datasets:\n",
    "            print(str(dataset))\n",
    "            print(dataset.meta['loader'])\n",
    "            assert \"subsample\" not in dataset.meta['loader']\n",
    "\n",
    "            for percentage in percentages:\n",
    "                print(f'  - {int(percentage * 100):3d}%', f'{seed=}')\n",
    "                sub_ds = dataset.tasks_subsample_kgc(\n",
    "                    seed=seed,\n",
    "                    percentage_val=percentage,\n",
    "                )\n",
    "\n",
    "                report = run(\n",
    "                    sub_ds,\n",
    "                    name='true-mentions',\n",
    "                    model=MODELS['true-mentions'],\n",
    "                    split='validation',\n",
    "                    seed=seed,\n",
    "                )\n",
    "\n",
    "                flat = flatten(report)\n",
    "\n",
    "                if writer is None:\n",
    "                    header = ['percentage', 'head tasks', 'tail tasks', 'seed'] + list(flat.keys())\n",
    "                    writer = csv.DictWriter(fd, fieldnames=header)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                writer.writerow(flat | {\n",
    "                    'percentage': percentage,\n",
    "                    'head tasks': len(sub_ds.open_kgc_val_heads),\n",
    "                    'tail tasks': len(sub_ds.open_kgc_val_tails),\n",
    "                    'seed': seed\n",
    "                })\n",
    "\n",
    "\n",
    "def subsample_experiments(datasets_config, percentages, seed):\n",
    "    name = datasets_config['config_file'].stem\n",
    "    fname = f\"subsample-experiments-{name}.csv\"\n",
    "\n",
    "    run_subsampling(\n",
    "        out=p_data / \"evaluation\" / fname,\n",
    "        datasets_config=datasets_config,\n",
    "        percentages=percentages,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "subsample_experiments(\n",
    "    datasets_config={\n",
    "        'config_file': irt2.ENV.DIR.CONF / 'datasets' / 'original.yaml',\n",
    "        'only': ['irt2/tiny'],\n",
    "    },\n",
    "    # irt2.ENV.DIR.CONF / 'datasets' / 'full.yaml',\n",
    "    percentages=(\n",
    "        [x/100 for x in range(1, 10)] +\n",
    "        [x/100 for x in range(10, 40, 5)] +\n",
    "        [x/100 for x in range(40, 101, 20)]\n",
    "    ),\n",
    "    seed=31189,\n",
    ")\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "name": "control-experiments.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
